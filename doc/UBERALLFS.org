#+TITLE: Uberallfs
#+AUTHOR: Christian Thäter
#+EMAIL: ct@pipapo.org
#+LANGUAGE: en
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper, hidelinks]
#+LATEX_HEADER: \usepackage{enumitem}
#+LATEX_HEADER: \setlist[description]{style=nextline}
#+LATEX_HEADER: \parskip8pt
#+LATEX_HEADER: \parindent0


* Overview

** The Idea

   Uberallfs is a peer to peer distributed filesystem. Objects are cached on the accessing
   node. Mutation is implemented by passing Tokens around. Only the owner of the Token is
   eligible to mutate the data. This token can be requested from the current token holder by any
   entity which has permission to mutate the object. The old Token holder will then keep a
   reference to the entity which received the token. These references can then be walked to
   find the authoritative node at the end of the list. The actual implementation of this Idea
   becomes a bit more complex and offers certain optimization opportunities.

   Entities who don't have 'write' access do an object will never get the token. When read
   access is requested they synchronize the object with the nodes who hold the original
   data. This synchronization is managed by the current token owner.

** Features and Goals

*** Caching and pinning objects
    Objects become cached upon access. There will be tools to enforce this caching and pin
    objects to a node.

*** Offline use
    The filesystem caches objects and can be used when a node is offline. For writes this
    needs either the token to be locally obtained or the object can become 'detached' and
    changes need to be merged when connectivity is restored (which can be automatic if there
    are no conflicts).

*** Strong security and anonymity available
    Objects can be either instantiated by a 'creator' who defines security policies about who
    has access to them OR published as anonymous immutable.

*** mixed in local data
    Objects may be local only and never be shared.

*** redundant storage
    Different levels of redundancy are planned, raid1 like redundancy where sync/close calls
    only complete after the data is replicated or lazy backup schemes where data becomes
    syncronized at lower priority without blocking current access.

*** Garbage collection / Balancing
    When space becomes scarce unused objects can be evicted from the cache. Either if this is
    just a copy (that is not used for redundancy) by deleting the object or by offering
    objects to other nodes within a configured realm of hosts.

*** Striped parallel downloads
    If possible (later) Object transfer and syncronization can be spread over multiple peers
    to utilize better bandwidth sharing (Bittorrent alike).



** Design Choices
   Uberallfs uses 'opinionated' Design. Protocols include a single version number which fully
   defines the properties, sizes and algorithms used. Future versions will be backward
   compatible to few older versions but eventually old versions will become unsupported (which
   may happen earlier when there are security related problems).

   Version O is always defined to be 'experimental' it will be used in closed environments for
   testing and development, never in production. Any Version 0 Protocol outside of this
   environment is considered incompatible with itself.

* Components

  Following a coarse overview of the components making uberallfs. Details are described in
  later Chapters.

** Object Store

   At the core is a object store where all filesystem objects are cached. Later support for
   volatile objects is planned to allow once used streaming data. [[#bd6e60d2-31a6-46f8-87ec-173f395ef49b][For Details see below.]]

** Frontends

   User-access to the underlying filesystem hierarchy. The primary goal is a Linux fuse
   filesystem which maps the underlying uberallfs to an ordinary POSIX conforming filesystem.

   Later other front ends are planned. Android storage framework for example.

** Object Discovery

   As described in the introduction, the 'trail' pointer used to locate the node which is
   authoritative for a filesystem object is the main concept of uberallfs. Still there needs
   to be more to make this functional. For example Objects need to be recovered when the trail
   got broken (lost node). Only nodes which have full access to an object are allowed to
   become authoritative.

   When a node becomes authoritative this does not mean that the data is available there, it
   only manages the 'ownership'. The object metadata contains references to nodes who
   actually hold the data. For reading the data will be synchronized. While writing only
   invalidates the old references and instantiates new data locally.

   Nodes without full access to objects can synchronize data as far they have permissions to
   do so and negotiate promises and leases with the authoritative node for race free data
   access.

** Object Synchronization

   Once access/authority to an object is granted the data may be synchronized (for reads).
   For this maps of byte-ranges and version/generation counts are used. There is no need for
   rsync like checksumming since the authoritative always knows which data is changed/recent.

   Objects may become scattered across the nodes when frequent random writes at different
   locations of an object happen. This is mitigated by a low priority object coalescing which
   gather fragments and merges them on single nodes.

** Access Control

   Access control is implemented over public keys and signatures. The node which is
   authoritative over an object is responsible for enforcing the permissions. Access control
   metadata is sufficient enough to be freestanding without any additional information. Still
   due to the distributed nature there are some loopholes that can not be closed (discussed
   below). Basically any access ever granted can not be reliably revoked at a later time.

   [[#62c4e059-5538-48a1-953a-43c1c9a5d7fb][Details below.]]

   planned: eventually a special tree object which holds revoked signatures, must be safe
   against DoS, needs some thinking.

** Network / Sessions

   A node establishes a session with another node on behalf of a user/key. Each session is
   then authenticated for this keys which is used for access control. Sessions are keep state
   for some operations. As long a session is alive these states are valid. When a session dies
   unexpectedly then these states and all associated data gets cleaned up/rolled back.

** Node Discovery

   Nodes are addressed by their public keys. The last seen addresses and names of other nodes
   are cached for fast lookup. If that fails then a discovery is initiated (Details to be
   worked out).

** Distributed PKI

   Future versions will include a distributed public key infrastructure. This augments the
   exiting Access control with more advanced features like:
    - web of trust for confirming identity and credibility of other keys
    - revoking signatures
    - key aliasing/delegation
    - key renewal.

* Object Store
  :PROPERTIES:
  :CUSTOM_ID: bd6e60d2-31a6-46f8-87ec-173f395ef49b
  :END:

  While uberallfs looks like a hierarchical filesystem, the backend store is a flat key/value
  object store. The keys are derived from universally unique and secure identifiers. Secure in
  this context means that not entity can create a collision that goes unnoticed. These
  identifiers resemble global unique inode numbers.

  There are different object types of objects stored under a key, explained later in this
  document. The main parts are the 'tree' and 'blob' types. A 'tree' is an object that holds
  named references to sub-object keys much like a directory in a filesystem. Blob objects
  contain the file data. Other types contain metadata for security and distribution.

  A mounted uberallfs uses a 'tree' object as the root of the mountpoint. From
  there on a hierarchy like with any other filesystem is created.

  The difference here is that all objects can be distributed over the network and anyone (with
  permission to access the object) can references them within his own hierarchy. This for
  example allows a complete home directory to be shared as well as mounting the same object
  (directory) under different names at different positions in the hierarchy. For example one
  instance may name a directory './Work/' and another one refers to the same tree object as
  './Arbeit/'.

  Eventually (if one is careless) this could lead to directory cycles, which is the major
  difference to traditional filesystems where directory cycles are highly disregarded.

** Identifier Types

   A mutable objects are identified by a unique (random) number while an immutable object is
   identified by a hash over its content. Objects which are constrained by permissions a
   digital signature is required to guarantee integrity (see below).

   We can further deduce the necessity of 3 scopes where these keys are valid:
   1. private objects that must never be shared but is accessible to the local instance
   2. public objects that have ownership and access permissions
   3. anonymous objects without any ownership and public access

   This leads to following 4 types of identifiers:

   |           | private | public           | anonymous |
   |-----------+---------+------------------+-----------|
   | mutable   | random  | random signature | ¹         |
   | immutable | ²       | hash signature   | hash      |

   Note that there are 2 not supported combinations:
   1. Anonymous mutable data would lead security problems like denial of service attacks
   2. Having immutable private objects won't have any security implications and may be
      supported at some point when need arises (eg. deduplication)

   Eventually some more Types might be supported, for example hashing could be indirect being
   the hash over a bittorrent like list of hashes. This may even become the default for
   immutable objects at some point.

** Object Types

   Details explained in the next chapter.

*** tree
    Stores references to other objects (trees, blobs, symlinks) May store Unix special files
    (fifo, sockets, device nodes) initially private, eventually network transparent nodes may
    be implemented.

*** blob
    The actual object (file) data.

*** perm
    Security manifest, access control and security related metadata.

*** meta
    Extra metadata about authority/trail/generation/distribution.

*** dmap
    Maps to the nodes holding the data for mutable files. Initially only complete objects,
    later byte ranges/multi node.

*** hash
    Torrent like hash list for immutable files.



* Access Control
  :PROPERTIES:
  :CUSTOM_ID: 62c4e059-5538-48a1-953a-43c1c9a5d7fb
  :END:

  The 'perm' object type contains all metadata necessary for access control for the associated object. Any
  node is obliged to validate access rights on queries.

  - Identification ::

    We must ensure that an Object Key and Identifier belongs to the Object in question and
    all following security metadata needs to be derived from this in a provable way. All
    public keys can be constrained by an expire date.

    - Identifier ::
      A random number.
    - Creator ::
      Public key of the Creator/expiration of this object. Can be only once used key which is
      deleted after initialization of the metadata. The expiration date here becomes part of
      the identifier. Once passed the object becomes invalid and can be purged.
    - Identifier Signature ::
      The Identifier is signed with the Creators key.
    - Object Key ::
      The Identifier and its Signature are hashed together to give the key used in the
      object store. This is not stored in the 'perm' object as it is the 'name' thereof
      itself.

  - Administrative Lists ::
    - Super Admins ::
      A (optional) list of public key/expire tupes that are allowed to modify the
      per-permission admins below.
      - Super Admins Signature ::
        The list of Super-Admins together with a nonce and the Identifier becomes signed by
        the Creator. This indirection allows to dispose the Creator key now and to delegate
        administrative task to multiple entities. Caveat: after the Creator key is disposed
        the Super-Admin list can not be changed anymore.

    - Per Permission Admins ::
      Optional list for each possible permission (read, write, delete, append, ...). Keys
      listed in these lists are allowed to modify the respective ACL's below. (idea:
      permission tags on the lists itself: an admin may add/delete...)
      - Per Permission Admins Signature ::
        Each of the lists above needs to be signed by the Creator or a Super-Admin.
        This signature contains a nonce and the Identifier as well

  - Access Control Lists ::
    Optional list for each possible permission (read, write, delete, append, ...). Keys
    listed in these lists are allowed to access the object in requested way.
    - ACL Signature ::
      Each of the lists above needs to be signed by the Creator or a Super-Admin or a
      matching per-permission-Admin. This signature contains a nonce and the Identifier as
      well.

  - Generation Count and Signature ::
    Whenever any data on the above got changed a generation counter is incremented and the
    all list blocks plus this generation counter must be signed by one of the above
    administrative Keys (usually the one who did the change).


** Brainstorm/Ideas

   - Quorum :: M of N Admins must grant permission to be effective

** Security Implications

   TBD: in short one who once had (administrative) access to the object can replay that old
   version of the metadata under some conditions since the 'trail' and generation count can
   be incomplete. (write example how this can happen, any solution for this?)

   1. A creates a file with B and C as Admin
   2. B takes the token from A   A->B
   3. C takes the token from B   A->B->C
   4. C removes B from an Administrative list
   5. B takes the token from C back  A->B<-C
   6. B replays the 'perm' metadata from 2. (gains Admin back)
   7. A takes the file from B but can not discover the tampering

   The only 'weak' protection against this are the expiration dates. When these are short
   enough they limit the time window in which such an attack can be done and constrain the
   necessary lifetime for signature revocations.

*** malicious object mutation

    Can not happen because the token will never be given to a node that won't have write access.

      
*** privilege escalation


      
*** Object persistence
      

    
 
